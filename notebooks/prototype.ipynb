{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = tf.constant(np.load('./data/squad/glove.trimmed.100.npz')['glove'], name='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = open('./data/squad/train.ids.context', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = a.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = map(int, lines[0].strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_mask(x):\n",
    "    return tf.ones_like(x, dtype=tf.int32)\n",
    "\n",
    "def make_dataset(filenames, batch_size=10, epoch_size=2, with_mask=False):\n",
    "    dataset = tf.contrib.data.TextLineDataset(filenames)\n",
    "    \n",
    "    dataset = (dataset.map(lambda line: tf.string_split([line]).values)\n",
    "                    .map(lambda strings: tf.string_to_number(strings, out_type=tf.int32)))\n",
    "    if with_mask:\n",
    "        dataset = dataset.map(lambda x: (x, build_mask(x)))\n",
    "                    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset1 = make_dataset(['./data/squad/train.ids.context'], with_mask=True)\n",
    "dataset2 = make_dataset(['./data/squad/train.ids.question'], with_mask=True)\n",
    "dataset3 = make_dataset(['./data/squad/train.span'])\n",
    "\n",
    "dataset = tf.contrib.data.Dataset.zip((dataset1, dataset2, dataset3))\n",
    "dataset = (dataset.padded_batch(10, padded_shapes=(([None], [None]), ([None], [None]), ([None])))\n",
    "                    .repeat(2)\n",
    "                    .shuffle(buffer_size=10000))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "context_tuple, question_tuple, span = iterator.get_next()\n",
    "context, cmask = context_tuple\n",
    "question, qmask = question_tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_embed = tf.nn.embedding_lookup(embeddings, context)\n",
    "question_embed = tf.nn.embedding_lookup(embeddings, question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import rnn_cell_impl\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as _bidirectional_dynamic_rnn\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "\n",
    "def flatten(tensor, keep):\n",
    "    fixed_shape = tensor.get_shape().as_list()\n",
    "    start = len(fixed_shape) - keep\n",
    "    left = reduce(mul, [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start)])\n",
    "    out_shape = [left] + [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start, len(fixed_shape))]\n",
    "    flat = tf.reshape(tensor, out_shape)\n",
    "    return flat\n",
    "\n",
    "\n",
    "def reconstruct(tensor, ref, keep):\n",
    "    ref_shape = ref.get_shape().as_list()\n",
    "    tensor_shape = tensor.get_shape().as_list()\n",
    "    ref_stop = len(ref_shape) - keep\n",
    "    tensor_start = len(tensor_shape) - keep\n",
    "    pre_shape = [ref_shape[i] or tf.shape(ref)[i] for i in range(ref_stop)]\n",
    "    keep_shape = [tensor_shape[i] or tf.shape(tensor)[i] for i in range(tensor_start, len(tensor_shape))]\n",
    "    # pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))]\n",
    "    # keep_shape = tensor.get_shape().as_list()[-keep:]\n",
    "    target_shape = pre_shape + keep_shape\n",
    "    out = tf.reshape(tensor, target_shape)\n",
    "    return out\n",
    "\n",
    "\n",
    "def linear(args, output_size, bias, bias_start=0.0, scope=None, squeeze=False, wd=0.0, input_keep_prob=1.0,\n",
    "           is_train=None):\n",
    "    if args is None or (nest.is_sequence(args) and not args):\n",
    "        raise ValueError(\"`args` must be specified\")\n",
    "    if not nest.is_sequence(args):\n",
    "        args = [args]\n",
    "\n",
    "    flat_args = [flatten(arg, 1) for arg in args]\n",
    "    if input_keep_prob < 1.0:\n",
    "        assert is_train is not None\n",
    "        flat_args = [tf.cond(is_train, lambda: tf.nn.dropout(arg, input_keep_prob), lambda: arg)\n",
    "                     for arg in flat_args]\n",
    "    flat_out = rnn_cell_impl._linear(flat_args, output_size, bias, \n",
    "                                     bias_initializer=tf.zeros_initializer())\n",
    "    out = reconstruct(flat_out, args[0], 1)\n",
    "    if squeeze:\n",
    "        out = tf.squeeze(out, [len(args[0].get_shape().as_list())-1])\n",
    "    if wd:\n",
    "        add_wd(wd)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def highway_layer(arg, bias, bias_start=0.0, scope=None, wd=0.0, input_keep_prob=1.0, is_train=None):\n",
    "    with tf.variable_scope(scope or \"highway_layer\"):\n",
    "        d = arg.get_shape()[-1]\n",
    "        with tf.variable_scope('trans'):\n",
    "            trans = linear([arg], d, bias, bias_start=bias_start, scope='trans', wd=wd, input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "            trans = tf.nn.relu(trans)\n",
    "        with tf.variable_scope('gate'):\n",
    "            gate = linear([arg], d, bias, bias_start=bias_start, scope='gate', wd=wd, input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "            gate = tf.nn.sigmoid(gate)\n",
    "        out = gate * trans + (1 - gate) * arg\n",
    "        return out\n",
    "\n",
    "def highway_network(arg, num_layers, bias, bias_start=0.0, scope=None, wd=0.0, input_keep_prob=1.0, is_train=None):\n",
    "    with tf.variable_scope(scope or \"highway_network\"):\n",
    "        prev = arg\n",
    "        cur = None\n",
    "        for layer_idx in range(num_layers):\n",
    "            cur = highway_layer(prev, bias, bias_start=bias_start, scope=\"layer_{}\".format(layer_idx), wd=wd,\n",
    "                                input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "            prev = cur\n",
    "        return cur\n",
    "    \n",
    "# TODO do you need this?   \n",
    "class SwitchableDropoutWrapper(DropoutWrapper):\n",
    "    def __init__(self, cell, is_train, input_keep_prob=1.0, output_keep_prob=1.0,\n",
    "             seed=None):\n",
    "        super(SwitchableDropoutWrapper, self).__init__(cell, input_keep_prob=input_keep_prob, output_keep_prob=output_keep_prob,\n",
    "                                                       seed=seed)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        outputs_do, new_state_do = super(SwitchableDropoutWrapper, self).__call__(inputs, state, scope=scope)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        outputs, new_state = self._cell(inputs, state, scope)\n",
    "        outputs = tf.cond(self.is_train, lambda: outputs_do, lambda: outputs)\n",
    "        if isinstance(state, tuple):\n",
    "            new_state = state.__class__(*[tf.cond(self.is_train, lambda: new_state_do_i, lambda: new_state_i)\n",
    "                                       for new_state_do_i, new_state_i in zip(new_state_do, new_state)])\n",
    "        else:\n",
    "            new_state = tf.cond(self.is_train, lambda: new_state_do, lambda: new_state)\n",
    "        return outputs, new_state\n",
    "    \n",
    "def bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length=None,\n",
    "                              initial_state_fw=None, initial_state_bw=None,\n",
    "                              dtype=None, parallel_iterations=None,\n",
    "                              swap_memory=False, time_major=False, scope=None):\n",
    "    assert not time_major\n",
    "\n",
    "    flat_inputs = flatten(inputs, 2)  # [-1, J, d]\n",
    "    \n",
    "    print(inputs.get_shape(), flat_inputs.get_shape(), 'Flattening')\n",
    "    flat_len = None if sequence_length is None else tf.cast(flatten(sequence_length, 0), 'int64')\n",
    "\n",
    "    (flat_fw_outputs, flat_bw_outputs), final_state = \\\n",
    "        _bidirectional_dynamic_rnn(cell_fw, cell_bw, flat_inputs, sequence_length=flat_len,\n",
    "                                   initial_state_fw=initial_state_fw, initial_state_bw=initial_state_bw,\n",
    "                                   dtype=dtype, parallel_iterations=parallel_iterations, swap_memory=swap_memory,\n",
    "                                   time_major=time_major, scope=scope)\n",
    "\n",
    "    fw_outputs = reconstruct(flat_fw_outputs, inputs, 2)\n",
    "    bw_outputs = reconstruct(flat_bw_outputs, inputs, 2)\n",
    "    print(flat_fw_outputs.get_shape(), fw_outputs.get_shape(), 'Reconstruct')\n",
    "    # FIXME : final state is not reshaped!\n",
    "    return (fw_outputs, bw_outputs), final_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('context'):\n",
    "    context_embed = highway_network(context_embed, 2, True, wd=0.0, is_train=False)\n",
    "\n",
    "with tf.variable_scope('question'):\n",
    "    question_embed = highway_network(question_embed, 2, True, wd=0.0, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell import BasicLSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper\n",
    "\n",
    "# TODO the input_keep_prob should be 1.0 for inference\n",
    "cell = BasicLSTMCell(100, state_is_tuple=True)\n",
    "\n",
    "is_train = tf.constant(False)\n",
    "d_cell = SwitchableDropoutWrapper(cell, is_train, input_keep_prob=0.5)\n",
    "\n",
    "c_len = tf.reduce_sum(tf.cast(cmask, 'int32'), 1)  # [N]\n",
    "q_len = tf.reduce_sum(tf.cast(qmask, 'int32'), 1)  # [N]\n",
    "\n",
    "# TODO figure out why these embeds are in float64\n",
    "context_embed = tf.cast(context_embed, tf.float32)\n",
    "question_embed = tf.cast(question_embed, tf.float32)\n",
    "\n",
    "#TODO remove reuse for production\n",
    "with tf.variable_scope('prepro'):\n",
    "    (fw_u, bw_u), ((_, fw_u_f), (_, bw_u_f)) = _bidirectional_dynamic_rnn(d_cell, d_cell, question_embed, q_len, dtype='float', scope='u1') # [N, J, d], [N, d]\n",
    "\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    (fw_h, bw_h), _ = _bidirectional_dynamic_rnn(cell, cell, context_embed, c_len, dtype='float', scope='u1')  # [N, M, JX, 2d]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([10], dtype=int32)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([tf.shape(c_len)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = tf.concat([fw_u, bw_u], 2)\n",
    "h = tf.concat([fw_h, bw_h], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VERY_BIG_NUMBER = 1e30\n",
    "VERY_SMALL_NUMBER = 1e-30\n",
    "VERY_POSITIVE_NUMBER = VERY_BIG_NUMBER\n",
    "VERY_NEGATIVE_NUMBER = -VERY_BIG_NUMBER\n",
    "\n",
    "def exp_mask(val, mask, name=None):\n",
    "    \"\"\"Give very negative number to unmasked elements in val.\n",
    "    For example, [-3, -2, 10], [True, True, False] -> [-3, -2, -1e9].\n",
    "    Typically, this effectively masks in exponential space (e.g. softmax)\n",
    "    Args:\n",
    "        val: values to be masked\n",
    "        mask: masking boolean tensor, same shape as tensor\n",
    "        name: name for output tensor\n",
    "\n",
    "    Returns:\n",
    "        Same shape as val, where some elements are very small (exponentially zero)\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = \"exp_mask\"\n",
    "    return tf.add(val, (1 - tf.cast(mask, 'float')) * VERY_NEGATIVE_NUMBER, name=name)\n",
    "\n",
    "\n",
    "def linear_logits(args, bias, bias_start=0.0, scope=None, mask=None, wd=0.0, input_keep_prob=1.0, is_train=None):\n",
    "    with tf.variable_scope(scope or \"Linear_Logits\"):\n",
    "        logits = linear(args, 1, bias, bias_start=bias_start, squeeze=True, scope='first',\n",
    "                        wd=wd, input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "        if mask is not None:\n",
    "            logits = exp_mask(logits, mask)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "with tf.variable_scope('attention_layer'):\n",
    "    JQ = tf.shape(u)[1]\n",
    "    JX = tf.shape(h)[1]\n",
    "\n",
    "    h_aug = tf.tile(tf.expand_dims(h, 2), [1, 1, JQ, 1])\n",
    "    u_aug = tf.tile(tf.expand_dims(u, 1), [1, JX, 1, 1])\n",
    "\n",
    "    h_mask_aug = tf.tile(tf.expand_dims(cmask, 2), [1, 1, JQ])\n",
    "    u_mask_aug = tf.tile(tf.expand_dims(qmask, 1), [1, JX, 1])\n",
    "    hu_mask = tf.cast(h_mask_aug, tf.bool) & tf.cast(u_mask_aug, tf.bool)\n",
    "    hu_aug = h_aug * u_aug\n",
    "    u_logits = linear_logits([h_aug, u_aug, hu_aug], True, scope='u_logits', mask=hu_mask)\n",
    "    u_logits_reshaped = flatten(u_logits, 1)\n",
    "    u_softmax = tf.nn.softmax(u_logits_reshaped)\n",
    "    out = reconstruct(u_softmax, u_logits, 1)\n",
    "    target_rank = len(u_aug.get_shape().as_list())\n",
    "    u_a = tf.reduce_sum(tf.expand_dims(out, -1) * u_aug, target_rank - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_logits = tf.reduce_sum(u_logits, 2)\n",
    "h_logits_reshaped = flatten(h_logits, 1)\n",
    "h_softmax = tf.nn.softmax(h_logits_reshaped)\n",
    "out = reconstruct(h_softmax, h_logits, 1)\n",
    "target_rank = len(h.get_shape().as_list())\n",
    "h_a = tf.reduce_sum(tf.expand_dims(out, -1) * h, target_rank - 2)\n",
    "h_a = tf.tile(tf.expand_dims(h_a, 1), [1, JX, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p0 = tf.concat([h, u_a, h * u_a, h * h_a], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"main\"):\n",
    "    cell = BasicLSTMCell(100, state_is_tuple=True)\n",
    "    first_cell = SwitchableDropoutWrapper(cell, is_train, input_keep_prob=0.5)\n",
    "    (fw_g0, bw_g0), _ = _bidirectional_dynamic_rnn(first_cell, first_cell, p0, c_len, dtype='float', scope='g0')  # [N, JX, 2d]\n",
    "    g0 = tf.concat([fw_g0, bw_g0], 2)\n",
    "\n",
    "    cell = BasicLSTMCell(100, state_is_tuple=True)\n",
    "    first_cell = SwitchableDropoutWrapper(cell, is_train, input_keep_prob=0.5)\n",
    "    (fw_g1, bw_g1), _ = _bidirectional_dynamic_rnn(first_cell, first_cell, g0, c_len, dtype='float', scope='g1')  # [N, JX, 2d]\n",
    "    g1 = tf.concat([fw_g1, bw_g1], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = linear_logits([g1, p0], 100, 0.0, scope='logits1', mask=cmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(logits, mask=None, scope=None):\n",
    "    with tf.name_scope(scope or \"Softmax\"):\n",
    "        if mask is not None:\n",
    "            logits = exp_mask(logits, mask)\n",
    "        flat_logits = flatten(logits, 1)\n",
    "        flat_out = tf.nn.softmax(flat_logits)\n",
    "        out = reconstruct(flat_out, logits, 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "def softsel(target, logits, mask=None, scope=None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param target: [ ..., J, d] dtype=float\n",
    "    :param logits: [ ..., J], dtype=float\n",
    "    :param mask: [ ..., J], dtype=bool\n",
    "    :param scope:\n",
    "    :return: [..., d], dtype=float\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope or \"Softsel\"):\n",
    "        a = softmax(logits, mask=mask)\n",
    "        target_rank = len(target.get_shape().as_list())\n",
    "        out = tf.reduce_sum(tf.expand_dims(a, -1) * target, target_rank - 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO use batch _size\n",
    "a1i = softsel(tf.reshape(g1, [10, JX, 2 * 100]), tf.reshape(logits, [10, JX]))\n",
    "a1i = tf.tile(tf.expand_dims(a1i, 1), [1, JX, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_logits1 = tf.reshape(logits, [-1, JX])\n",
    "flat_yp = tf.nn.softmax(flat_logits1)  # [-1, M*JX]\n",
    "yp = tf.reshape(flat_yp, [-1, JX])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorShape([Dimension(10), Dimension(None), Dimension(1400)]), TensorShape([Dimension(10), Dimension(None), Dimension(1400)]), 'Flattening')\n",
      "(TensorShape([Dimension(10), Dimension(None), Dimension(100)]), TensorShape([Dimension(10), Dimension(None), Dimension(100)]), 'Reconstruct')\n"
     ]
    }
   ],
   "source": [
    "cell = BasicLSTMCell(100, state_is_tuple=True)\n",
    "d_cell = SwitchableDropoutWrapper(cell, is_train, input_keep_prob=0.5)\n",
    "(fw_g2, bw_g2), _ = bidirectional_dynamic_rnn(d_cell, d_cell, tf.concat([p0, g1, a1i, g1 * a1i], 2),\n",
    "                                                          c_len, dtype='float', scope='g2')  # [N, M, JX, 2d]\n",
    "g2 = tf.concat([fw_g2, bw_g2], 2)\n",
    "logits2 = linear_logits([g2, p0], 100, 0.0, scope='logits2', mask=cmask)\n",
    "flat_logits2 = tf.reshape(logits2, [-1, JX])\n",
    "flat_yp = tf.nn.softmax(flat_logits2)  # [-1, M*JX]\n",
    "yp2 = tf.reshape(flat_yp, [-1, JX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "span0, span1 = tf.split(span, num_or_size_splits=2, axis=1)\n",
    "\n",
    "loss1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(span0, [-1]), logits=flat_logits1, name='loss1')\n",
    "loss2 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(span1, [-1]), logits=flat_logits2, name='loss2')\n",
    "loss = tf.reduce_mean(tf.reshape(loss1, [-1, 1]) * tf.to_float(cmask)) + tf.reduce_mean(tf.reshape(loss2, [-1, 1]) * tf.to_float(cmask))\n",
    "#sess.run([tf.shape(tf.reshape(span0, [-1, 1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 10, 168, 200], dtype=int32),\n",
       " array([ 10, 168, 200], dtype=int32),\n",
       " array([ 10, 168], dtype=int32),\n",
       " array([ 10, 168], dtype=int32),\n",
       " 7.1617618,\n",
       " array([10], dtype=int32)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run([tf.shape(g0), tf.shape(g1), tf.shape(logits), tf.shape(yp), loss, tf.shape(loss2)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
